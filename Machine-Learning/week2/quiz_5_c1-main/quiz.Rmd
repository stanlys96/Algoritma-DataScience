---
title: "Classification in Machine Learning I Quiz"
author: "Team Algoritma"
date: "`r format = Sys.Date('%e, %B %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
    theme: united
    highlight: zenburn
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      mmessage = FALSE,
                      warning = FALSE,
                      fig.align = 'center',
                      comment = '#>')
options(scipen = 999)
```

# Classification 1 Quiz

This quiz is part of Algoritma Academy assessment process. Congratulations on completing the Classification in Machine Learning I! We will conduct an assessment quiz to test the practical classification model techniques that you have learned on the course. The quiz is expected to be taken in the classroom, please contact our team of instructors if you missed the chance to take it in class.

To complete this assignment, you are required to build a classification model to classify whether employees will resign or not based on their characteristics. Use Logistic Regression and k-Nearest Neighbor algorithms by following these steps:

# Method Understanding

Classification is a predictive method to predict a target variable of categorical type in *supervised machine learning*. Classification models can be divided into 2 types; a model that can be interpreted such as *logistic regression* and model that is robust and can not be interpreted such as *k-nearest neighbor*.

One application of the classification model in the financial industry is credit scoring. Credit scoring is useful to determine whether a customer is worthy of a loan or not. The data that is usually used are customer (prospective borrowers) profile data, customer demographics, income, employment status, income status, total income, homeownership status, and many more. The more complete the information, the more accurate the calculations in making the model may be.

Imagine a financial company that wants to streamline its credit scoring process using *machine learning*. The company wants to get a model with a good performance and can be used to determine the contribution of each variable used to assess whether a customer is eligible for a loan or not. The chosen model will later be used as a reference in giving out loan. It is hoped that the use of the model can reduce the risk of loss due to errors in lending to customers who will not be able to repay the loan. 

1. Based on the problem above, which of the following statements is **not true**?
- [ ] the variables that can be used as predictors are income, customer demographics, and employment status
- [ ] the target variable is credit status assessment (whether the customer will be able to pay/unable to pay for the loan)
- [ ] the more appropriate model to use is the k-nearest neighbor
- [ ] this case can be solved using a classification model

Besides the cases above, there are many more cases that can be solved using the classification method in various business lines. One of them is from the field of human resources (HR). Please solve the case below by applying the classification method that you have learned in the class. 

# Data Exploration

Let us start by preparing and exploring the data first. In this quiz, you will be using the employee turnover data (`turnover`). The data is stored in a .csv format in this repository as `turnover_balance.csv`. Import your data using `read.csv` or `read_csv` and save as `turnover` object. Before building your classification model, you will need to perform an exploratory analysis to understand the data. Glimpse the structure of our `turnover` data! You can choose either `str()` or `glimpse()` function.

```{r}
# your code here
turnover <- read.csv("turnover_balance.csv")
```

Turnover data consists of 10 variables and 7142 rows. This is a Human Resource Management data that shows historical data of employee characteristics who have resigned or not. Below is the detailed information about each variable in the dataset:

  - `satisfaction_level`: the level of employee satisfaction working in a company
  - `last_evaluation`: employee satisfaction level at the last evaluation
  - `number_project`: the number of projects the employee has received
  - `average_monthly_hours`: average hours worked per month
  - `time_spend_company`: length of time in the company (years)
  - `Work_accident`: presence or absence of work accident, 0 = none, 1 = exist
  - `promotion_last_5years`: ever got a promotion in the last 5 years, 0 = no, 1 = yes
  - `division`: name of department or division
  - `salary`: income level, divided into low, medium and high
  - `left`: employee resignment data, 0 = no, 1 = yes
  
In this quiz, we will try to predict whether or not the employee has a resignation tendency using the `left` column as our target variable. Please change the class of `Work_accident`, `promotion_last_5years`, `division`, `salary` and `left` column to be in factor class as it should be.

```{r}
# your code here
library(dplyr)
turnover <- turnover %>% mutate(Work_accident = as.factor(Work_accident), promotion_last_5years = as.factor(promotion_last_5years), division = as.factor(division), salary = as.factor(salary), left = as.factor(left))
turnover %>% group_by(division) %>% summarise(the_mean = mean(average_monthly_hours)) %>% arrange(desc(the_mean))
```


For example, as an HR staff, we are instructed to investigate divisions that has a long history of an employee resigning and analyze their average monthly hours. Let's do some aggregation of `average_monthly_hours` for each division. Because you only focused on the employee who left, you should filter the historical data with the condition needed. 

Using **dplyr** functions, you can use `filter()`, then `group_by()` function by `division` and `summarise()` the mean of `average_monthly_hours`, then arrange it based on `average_monthly_hours` from high to low using `arrange()` function.

As an alternative, if you are more familiar using **base R** code style, you can filter the data using conditional subsetting `data["condition needed",]`, than assign it into `df_left` object. After that, you can aggregate `df_left` based on `division` and `average_monthly_hours` column using `aggregate()` function. Don't forget to use `mean` in `FUN` parameter and assign it into `df_agg`. In order to get the ordered mean value from high to low of the `average_monthly_hours`, you can use `order()` function in conditional subsetting `data[order(column_name, decreasing = T), ]`.


```{r}
# your code here

```
___
2. Based on the aggregation data that you have analyzed, which are the top 3 divisions with the highest average of monthly hours?
  - [ ] Marketing, Accounting, Management
  - [ ] Accounting, Support, Sales
  - [ ] Technical, IT, Management
  - [ ] Technical, IT, Research and Development (RandD)
___

# Data Pre-processing

After conducting the data exploration, we will perform pre-processing steps before building the classification model. Before we build the model, let us take a look at the proportion of our target variable in the `left` column using `prop.table(table(data))` function.

```{r}
# your code here
table(turnover$left) %>% 
  prop.table()
```

It seems like our target variable has a balanced proportion between both classes. Before we build the model, we should split the dataset into train and test data in order to perform model validation. Split `turnover` dataset into 80% train and 20% test using `sample()` function and use `set.seed()` with the seed 100. Store it as a `train` and `test` object.

> **Notes:** Make sure you use `RNGkind()` and `set.seed()` before splitting data and run it together with your `sample()` code

```{r}
library(rsample)
RNGkind(sample.kind = "Rounding")
set.seed(100)
# your code here
splitter <- initial_split(data = turnover, prop = 0.8)

# splitting
train <- training(splitter)
test <- testing(splitter)
```

Let's take a look at the distribution of our target variable in `train` data using `prop.table(table(data))` to make sure that the train data also have a balanced proportion of our target class. Please round the proportion to two decimal places using the `round()` function.

```{r}
# your code here
table(train$left) %>% 
  prop.table()
```

___
3. Based on the result above, which statement below is most fitting?
  - [ ] The class distribution is not balanced, but it is not necessary to balance the class proportion.
  - [ ] The class distribution is balanced, but it is not necessary to balance the class proportion.
  - [ ] The class distribution is balanced, but we should also make sure that the test data set also have balanced proportion.
  - [ ] The class distribution is balanced, and it is important to balance the class proportion so that model can predict well in both classes.
___

# Logistic Regression Model Fitting

After we have split our dataset into train and test set, let's try to model our `left` variable using all of the predictor variables to build a logistic regression. Please use the `glm(formula, data, family = "binomial")` to do that and store your model under the `model_logistic` object. Remember, we are not using `turnover` dataset any longer, and we will be using `train` dataset instead.

```{r}
# model_logistic <- glm()
model_all <- glm(
  formula = left ~ .,
  data = train,
  family = "binomial"
)
summary(model_all)
```

Based on the `model_logictic` you have made above, take a look at the summary of your model using `summary()` function.

```{r}
# your code here
exp(1.5014787)
```

___
4. Logistic regression is one of the interpretable models. We can explain how likely each variable predicts the class we observe. Based on the model summary above, what can be interpreted from the `salarymedium` coefficient?
  - [ ] The probability of an employee that received medium salary to resign is 1.50.
  - [ ] Employee who received medium salary is about 4.48 more likely to resign than the employee who received the other levels of salary.
  - [ ] Employee who received medium salary is about 4.48 more likely to resign than the employee who received high salary.
___

# K-Nearest Neighbor Model Fitting

Now let's try to explore the classification model using the k-Nearest Neighbor algorithm. In the k-Nearest Neighbor algorithm, we need to perform one more step of data pre-processing. For both our `train` and `test` set, drop the categorical variable from each data except our `left` variable. Separate the predictor and target from our `train` and `test` set.

```{r}
# predictor variables in `train`
train_x <- train %>% select(-left)

test_x <- test %>% select(-left)

# target
train_y <- train$left

test_y <- test$left
```

Recall that the distance calculation for kNN is heavily dependent upon the measurement scale of the input features. Any variable that has an extremely different range of values from the other variable can potentially cause problems for our classifier. So let's apply normalization techniques to rescale the features to a standard range of values.

To normalize the features in `train_x`, please use `scale()` function. Meanwhile, for the testing data, please normalize each features using the attributes *center* and *scale* obtained from the `train_x` data.

Please look up to the following code as an example to normalize the `train_x` and `test_x` data:

```{r eval=FALSE}
# DON'T RUN THIS CODE
# train
train_scaled <- scale(train)
# test
test_scaled <- scale(test, 
      center = attr(train_scaled, "scaled:center"),
      scale = attr(train_scaled, "scaled:scale"))
```

Now it's your turn to try it in the code below:

```{r}
# your code here

# scale train_x data
train_x <- scale(train_x)

# scale test_x data
test_x <- scale(test_x,
                        center = attr(credit_train_xs, "scaled:center"),
                        scale = attr(credit_train_xs, "scaled:scale"))

```

After normalizing our data, we need to find the right **K** to use for our kNN model. In practice, choosing `k` depends on the complexity of the data that needs to be learned and the number of records in the training data.

___
5. The method for getting `k` value does not guarantee you to get the best result. But, there is one common practice for determining the number of `k`. What method can we use to choose the number of `k`?
  - [ ] use k = 5
  - [ ] number of row
  - [ ] square root by number of row 
___

After answering the questions above, please find the number of `k` in the following code:

**Hint:** If you got a decimal number, do not forget to round it.

```{r}
# your code here
prob_value <- predict(
  object = model_all,
  newdata = test,
  type = "response"
)

test$prob_value <- predict(model_all, 
                          newdata = test, 
                          type = "response") #probability

test$pred_label <- ifelse(test$prob_value > 0.55, "1", "0")

library(caret)

confusionMatrix(
  data = as.factor(test$pred_label),
  reference = test$left,
  positive = "1"
)
```

Using `k` value we have calculated in the section before, try to predict `test_y` using `train_x` dan `train_y` dataset. Please use the `knn()` function and store the result under the `model_knn` object. Use the following code to help you:

```{r}
library(class)
sqrt(nrow(train_x))
model_knn <- knn(train = ______, test = ________, cl = _______, k = sqrt(nrow(train_x)))
```

# Prediction

Now, let's get back to our `model_logistic`. In this section, try to predict the `test` data using `model_logistic` and return the probability value using `predict()` function with `type = "response"` in the parameter function and store it under `prob_value` object.

```{r}
prob_value <-
```

Because the prediction results from the logistic model are probabilities, we have to change them to categorical / class according to the target class we have. Now, given a threshold of 0.55, try to classify whether or not an employee will resign. Please use `ifelse()` function and store the prediction result under the `pred_value` object.

```{r}
pred_value <- 
```

Based on the prediction value above, try to answer the following question.

___
6. In the prescriptive analytics stage, the prediction results from the model will be considered for business decision-making. So, please take your time to check the prediction results. How many predictions do our `model_logistic` generate for each class?
  - [ ] class 0 = 614, class 1 = 815
  - [ ] class 0 = 717, class 1 = 712
  - [ ] class 0 = 524, class 1 = 905
 ___ 

# Model Evaluation

In the previous sections, we have performed a prediction using both Logistic Regression and kNN algorithm. However, we need to validate whether or not our model did an excellent job of predicting unseen data. In this step, try to make a confusion matrix from the logistic regression result based on the actual label from `test` data and the predicted label (`pred_value`) and use the positive class as "1".

**Note:** Do not forget to do the explicit coercion `as.factor()` if your data is not yet stored as factor type.

```{r}
library(caret)
# your code here

```

Make the same confusion matrix for `model_knn` prediction result and `test_y`.

```{r}
# your code here

```

Let's say that we are working as an HR staff in a company and are utilizing this model to predict the probability of an employee resigning. As HR, we would want to know which employee is highly potential to resign so that we can take a precautionary approach as soon as possible. 

As a side note, the company prefers to have most of the predictions correct than to detect as many possible resigning employees. This is to avoid unnecessary budget for the falsely predicted resigning employees.

Now try to answer the following questions.

___
7. Which one is the right metric to evaluate the model performance based on the business case explained above?
  - [ ] Recall
  - [ ] Specificity  
  - [ ] Accuracy  
  - [ ] Precision  

___
8. Using the metrics of your choice from the previous question, which of the two models has a better performance in detecting resigning employees?
  - [ ] K-Nearest Neighbor  
  - [ ] Logistic Regression
  - [ ] Both have more or less similar performance

___
9. Now, recall what we have learned about the advantage and limitation of each model. Which statement below is **NOT TRUE**?
  - [ ] Use kNN because it tends to have a higher performance than Logistic Regression and able to perform binary or multiclass classification.
  - [ ] Use Logistic regression because it is interpretable and can can process both numerical and categorical variables as predictor.
  - [ ] It is still better to use kNN than Logistic Regression to gain higher model performance, even when most of your predictor is categorical variables.
___
